{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sparqlwrapper: https://sparqlwrapper.readthedocs.io/en/latest/main.html#installation-distribution\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://kaiko.getalp.org/sparql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'en_cont_wordforms.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting query terms in the list\n",
    "\n",
    "en_cont_terms = []\n",
    "with open (\"cont_en.txt\", \"r\") as f:\n",
    "    for s in f:\n",
    "        en_cont_terms.append(s.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_cont_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first part of a query string that remains unchanged\n",
    "\n",
    "query_srting_1 = \"\"\"SELECT ?entry ?lemma_lit ?pos ?otherForm_lit\\n\\\n",
    "WHERE {\\n\\\n",
    "?entry a ontolex:LexicalEntry ;\\n\\\n",
    "lime:language \"en\" ;\\n\\\n",
    "lexinfo:partOfSpeech ?pos ;\\n\\\n",
    "ontolex:canonicalForm/ontolex:writtenRep ?lemma_lit ;\\n\\\n",
    "ontolex:otherForm / ontolex:writtenRep ?otherForm_lit .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# querying takes approx 18 min\n",
    "\n",
    "data_en = {} # the resulting data in dict\n",
    "# {\"query_term\": {\"lemmata\":[{\"lemma_URI\":\"*\",\"lemma\":\"*\",\"pos\":\"*\",\"wordforms\":[\"*\"]}]}}\n",
    "\n",
    "for term in en_cont_terms:\n",
    "    \n",
    "    # generating a query string for each term\n",
    "    \n",
    "    query_string = query_srting_1 + f\"\\nFILTER regex(?lemma_lit, '^{term}$', 'i')\\n\\\n",
    "FILTER (?pos = <http://www.lexinfo.net/ontology/2.0/lexinfo#noun> || \\\n",
    "?pos = <http://www.lexinfo.net/ontology/2.0/lexinfo#adjective>)\" + \"}\"\n",
    "    \n",
    "    sparql.setQuery(query_string)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    # the results are converted into df to group by 'entry_URI'\n",
    "    \n",
    "    data_df = {\"entry_URI\":[entry['entry']['value'] for entry in results['results']['bindings']],\n",
    "               \"lemma_lit\":[entry['lemma_lit']['value'] for entry in results['results']['bindings']],\n",
    "               \"pos\":[entry['pos']['value'].split(\"#\")[1] for entry in results['results']['bindings']],\n",
    "               \"otherForm_lit\":[entry['otherForm_lit']['value'] for entry in results['results']['bindings']]}\n",
    "\n",
    "    results_pd = pd.DataFrame(data_df)\n",
    "\n",
    "    lemmata = []\n",
    "\n",
    "    # shaping the resulting dataset\n",
    "    \n",
    "    for group in results_pd.groupby(\"entry_URI\"):\n",
    "            lemmata.append({\"lemma_URI\":group[0],\n",
    "                         \"lemma\":list(group[1]['lemma_lit'])[0],\n",
    "                         \"pos\":list(group[1]['pos'])[0],\n",
    "                            # also adding a canonical form to the wordforms \n",
    "                         \"wordforms\":list(group[1]['otherForm_lit'])+ [list(group[1]['lemma_lit'])[0]]})\n",
    "            \n",
    "    data_en[term] = {\"lemmata\":lemmata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting json\n",
    "\n",
    "with open (\"en_cont_wordforms.json\",\"w\") as outfile:\n",
    "    json.dump(data_en, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'nl_cont_wordforms.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all the query terms in one string\n",
    "# LexiconService takes a string with multiple terms\n",
    "\n",
    "all_labels = \"\" \n",
    "with open (\"cont_nl.txt\", \"r\") as f:\n",
    "    for label in f:\n",
    "        clean_label = label.replace('\\xad', '').rstrip(\"\\n\")\n",
    "        all_labels += f\"{clean_label},\"\n",
    "str_labels = all_labels.rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP request to LexiconService: getting lemmas of Dutch contentious terms\n",
    "\n",
    "url = 'https://sk.taalbanknederlands.inl.nl/LexiconService/lexicon/get_lemma'\n",
    "query = {'database': 'molex', # modern Dutch lexicon\n",
    "         'case_sensitive':'false',\n",
    "          'wordform': str_labels} # querying all the terms at once\n",
    "headers = {'Accept': 'application/json'} # request json format\n",
    "\n",
    "r = requests.get(url,params=query,headers=headers)\n",
    "lemmas = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the found lemmas' IDs\n",
    "\n",
    "list_of_lemma_ids = []\n",
    "for lemma in lemmas['lemmata_list']:\n",
    "    for i in lemma['found_lemmata']:\n",
    "        if i['pos'] == 'AA' or i['pos'] == 'NOU-C': # including only nouns and adjectives\n",
    "            list_of_lemma_ids.append(i['lemma_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_lemma_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP request to LexiconService: getting word forms by lemma IDs\n",
    "\n",
    "url = 'https://sk.taalbanknederlands.inl.nl/LexiconService/lexicon/get_wordforms_from_lemma_id'\n",
    "headers = {'Accept': 'application/json'} # request json format\n",
    "\n",
    "# making a dict of {\"lemma_id\": [wordforms]}\n",
    "\n",
    "lemmaID_wordforms = {}\n",
    "\n",
    "for i in list_of_lemma_ids:\n",
    "    \n",
    "    query = {'database': 'molex',\n",
    "             'case_sensitive':'false',\n",
    "             'lemma_id': i} # only one ID at a time\n",
    "    \n",
    "    l = requests.get(url,params=query,headers=headers)\n",
    "    wordforms = l.json()\n",
    "    lemmaID_wordforms[i] = wordforms['wordforms_list'][0]['found_wordforms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaping the final dataset\n",
    "\n",
    "data_nl = {}\n",
    "for i in lemmas['lemmata_list']:\n",
    "    for l in i['found_lemmata']:\n",
    "        if l['pos'] != 'AA' and l['pos'] != 'NOU-C': # removing verb lemma IDs (with no word forms queried)\n",
    "            i['found_lemmata'].remove(l)\n",
    "            \n",
    "        if l['lemma_id'] in lemmaID_wordforms: # matching word forms with lemmas IDs\n",
    "            l['wordforms'] = lemmaID_wordforms[l['lemma_id']]\n",
    "            l['dataset'] = 'int/molex' # adding info about the dataset\n",
    "        \n",
    "    data_nl[i['query_word']] = {\"lemmata\":i['found_lemmata']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting json\n",
    "\n",
    "with open('nl_cont_wordforms.json', 'w') as outfile:\n",
    "    json.dump(data_nl, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
